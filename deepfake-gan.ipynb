{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2 as cv\n\nimport os\nimport random\n\n# Image augmentations\nimport albumentations as A\n\n# NN\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom collections import defaultdict\n\nfrom tqdm.notebook import tqdm\n\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:53:55.477706Z","iopub.execute_input":"2021-12-06T05:53:55.478395Z","iopub.status.idle":"2021-12-06T05:53:58.319413Z","shell.execute_reply.started":"2021-12-06T05:53:55.478032Z","shell.execute_reply":"2021-12-06T05:53:58.318613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_FOLDER = '../input/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\nTEST_FOLDER = 'test_videos'\n\nTRAIN_PATH_PRE_FIX = os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)\nTEST_PATH_PRE_FIX = os.path.join(DATA_FOLDER, TEST_FOLDER)\n\ntrain_fnames, test_fnames = os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)), os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)) \ntrain_fnames.remove(\"metadata.json\")\n\nprint(f\"Train samples: {len(train_fnames)}\")\nprint(f\"Test samples: {len(test_fnames)}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T06:30:13.019301Z","iopub.execute_input":"2021-12-06T06:30:13.019614Z","iopub.status.idle":"2021-12-06T06:30:13.032587Z","shell.execute_reply.started":"2021-12-06T06:30:13.019559Z","shell.execute_reply":"2021-12-06T06:30:13.031586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the labels from json\nimport json\n\nf = open(os.path.join(TRAIN_PATH_PRE_FIX, 'metadata.json'))\ndata = json.load(f)\n\n# f2 = open(os.path.join(TEST_PATH_PRE_FIX, 'metadata.json'))\n# data_test = json.load(f)\n\ndef get_label(vid_fname, training=True):\n#     if not training:\n#         return data_test[vid_fname]['label']\n    return data[vid_fname]['label']","metadata":{"execution":{"iopub.status.busy":"2021-12-06T14:39:27.722884Z","iopub.execute_input":"2021-12-06T14:39:27.723191Z","iopub.status.idle":"2021-12-06T14:39:27.756917Z","shell.execute_reply.started":"2021-12-06T14:39:27.723141Z","shell.execute_reply":"2021-12-06T14:39:27.755767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see how many are fake vs. real\nreal_ct, fake_ct = 0, 0\nfor fname in data:\n    if get_label(fname) == 'REAL':\n        real_ct += 1\n    else:\n        fake_ct += 1\nprint(real_ct, fake_ct)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:53:58.463758Z","iopub.execute_input":"2021-12-06T05:53:58.46401Z","iopub.status.idle":"2021-12-06T05:53:58.47086Z","shell.execute_reply.started":"2021-12-06T05:53:58.463951Z","shell.execute_reply":"2021-12-06T05:53:58.470051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_subset(data, n_samples):\n    return random.sample(data, n_samples)\n\n# To play around with our model, start small and develop more samples from this subset\ntrain_fnames, test_fnames = get_data_subset(train_fnames, 10), get_data_subset(test_fnames, 10)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:53:58.474237Z","iopub.execute_input":"2021-12-06T05:53:58.474877Z","iopub.status.idle":"2021-12-06T05:53:58.480538Z","shell.execute_reply.started":"2021-12-06T05:53:58.474742Z","shell.execute_reply":"2021-12-06T05:53:58.479737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_frames(fname, num_frames=10, every_n_frames=None):\n    \n    # Docs: https://docs.opencv.org/4.5.3/d8/dfe/classcv_1_1VideoCapture.html\n    vid = cv.VideoCapture(fname)\n    \n    # Properties found in https://docs.opencv.org/4.5.3/d4/d15/group__videoio__flags__base.html#gaeb8dd9c89c10a5c63c139bf7c4f5704d\n    total_frames = int(vid.get(cv.CAP_PROP_FRAME_COUNT))\n    h, w = int(vid.get(cv.CAP_PROP_FRAME_HEIGHT)), int(vid.get(cv.CAP_PROP_FRAME_WIDTH))\n    \n    step = total_frames//num_frames\n    frame_nums = [i*step for i in range(num_frames)]\n    \n    # If specified frame-interval, try to approximately center the frame captures (generally establish equal offsets)\n    # Getting the exact center is not as important so don't mind the imperfect slice\n    if every_n_frames:\n        \n        # Take a frame every n frames\n        \n        frame_range = every_n_frames*num_frames\n        start = total_frames // 2 - frame_range // 2\n        \n#         print(\"Start Frame\", start)\n#         print(\"End Frame\", total_frames // 2 + frame_range // 2)\n        \n        frame_nums = [i*every_n_frames + start for i in range(num_frames)]\n        \n    out = np.empty((num_frames, h, w, 3), np.dtype('uint8'))\n    \n    # Get the frames at the specified frame_nums and add it to out\n    curr_frame, i = 0, 0\n    for frame in frame_nums:\n        \n        # Advance to correct frame\n        while curr_frame != frame:\n            boo = vid.grab()\n            curr_frame+=1\n            \n        # Get current frame and place it in out\n        vid.grab()\n        out[i] = vid.retrieve()[1]\n        i+=1\n    return out","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:53:58.482085Z","iopub.execute_input":"2021-12-06T05:53:58.482384Z","iopub.status.idle":"2021-12-06T05:53:58.493151Z","shell.execute_reply.started":"2021-12-06T05:53:58.482327Z","shell.execute_reply":"2021-12-06T05:53:58.492377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for fname in train_fnames[:10]:\n#     pic_arr = get_frames(DATA_FOLDER + \"/\" + TRAIN_SAMPLE_FOLDER + \"/\" + fname, every_n_frames=5)\n    \n#     # For each picture array\n#     curr_pic = 1\n#     for pic_vals in pic_arr:\n#         im = Image.fromarray(pic_vals)\n#         # Get rid of .mp4 at the end\n#         new_fname = fname[:-4] + \"_\" + str(curr_pic)+\"_\" + str(len(pic_arr)) + \".jpeg\"\n\n#         # Imaged are saved in the format fname_pic#_totalPic#.jpeg\n#         im.save(DATA_FOLDER + \"/\" + TRAIN_SAMPLE_FOLDER + \"/\" + new_fname )\n\n#         curr_pic += 1","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:53:58.494391Z","iopub.execute_input":"2021-12-06T05:53:58.494863Z","iopub.status.idle":"2021-12-06T05:53:58.504631Z","shell.execute_reply.started":"2021-12-06T05:53:58.494814Z","shell.execute_reply":"2021-12-06T05:53:58.503855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport matplotlib.pylab as plt\ntrain_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'\n#train_dir = TRAIN_SAMPLE_FOLDER\nfig, ax = plt.subplots(1,1, figsize=(15, 15))\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\nvideo_file = train_video_files[3]\n#video_file = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/afoovlsmtx.mp4'\ncap = cv.VideoCapture(video_file)\nsuccess, image = cap.read()\nimage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\ncap.release()   \nax.imshow(image)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\nax.title.set_text(f\"FRAME 0: {video_file.split('/')[-1]}\")\nplt.grid(False)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:53:58.505932Z","iopub.execute_input":"2021-12-06T05:53:58.506372Z","iopub.status.idle":"2021-12-06T05:53:58.518923Z","shell.execute_reply.started":"2021-12-06T05:53:58.506202Z","shell.execute_reply":"2021-12-06T05:53:58.517945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install deepface","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:53:58.520544Z","iopub.execute_input":"2021-12-06T05:53:58.521083Z","iopub.status.idle":"2021-12-06T05:54:16.713622Z","shell.execute_reply.started":"2021-12-06T05:53:58.520914Z","shell.execute_reply":"2021-12-06T05:54:16.712867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from deepface import DeepFace\nimport matplotlib.pylab as plt\ntrain_dir = '/kaggle/input/deepfake-detection-challenge/train_sample_videos/'\n\nfig, ax = plt.subplots(1,1, figsize=(15, 15))\n\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir)]\nvideo_file = train_video_files[6]\n\nimage = get_frames(video_file)\nimg = cv.cvtColor(image[5], cv.COLOR_BGR2RGB)\n\nax.imshow(img)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\n#ax.title.set_text(f\"FRAME 0: {video_file.split('/')[-1]}\")\nplt.grid(False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:54:16.715147Z","iopub.execute_input":"2021-12-06T05:54:16.715425Z","iopub.status.idle":"2021-12-06T05:54:23.56604Z","shell.execute_reply.started":"2021-12-06T05:54:16.715376Z","shell.execute_reply":"2021-12-06T05:54:23.561809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"face = DeepFace.detectFace(image[4], detector_backend = 'ssd', enforce_detection = False)\nplt.imshow(face)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:54:23.567127Z","iopub.execute_input":"2021-12-06T05:54:23.567366Z","iopub.status.idle":"2021-12-06T05:54:25.847722Z","shell.execute_reply.started":"2021-12-06T05:54:23.567328Z","shell.execute_reply":"2021-12-06T05:54:25.846774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Credit to https://www.kaggle.com/vaillant/dfdc-3d-2d-inc-cutmix-with-3d-model-fix#Calculate-ensembled-prediction-&-clamp for the modified ResNet architecture\ndef conv3x3x3(in_planes, out_planes, stride=1):\n    # 3x3x3 convolution with padding\n    return nn.Conv3d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\ndef downsample_basic_block(x, planes, stride):\n    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n    zero_pads = torch.Tensor(\n        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n        out.size(4)).zero_()\n    if isinstance(out.data, torch.cuda.FloatTensor):\n        zero_pads = zero_pads.cuda()\n \n    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n \n    return out\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n \n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3x3(planes, planes)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.downsample = downsample\n        self.stride = stride\n \n    def forward(self, x):\n        residual = x\n \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n \n        out = self.conv2(out)\n        out = self.bn2(out)\n \n        if self.downsample is not None:\n            residual = self.downsample(x)\n \n        out += residual\n        out = self.relu(out)\n \n        return out\n\nclass ResNet(nn.Module):\n \n    def __init__(self,\n                 block,\n                 layers,\n                 sample_size,\n                 sample_duration,\n                 num_classes=400):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv3d(\n            3,\n            64,\n            kernel_size=7,\n            stride=(1, 2, 2),\n            padding=(3, 3, 3),\n            bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(\n            block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(\n            block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(\n            block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool3d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n \n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n \n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv3d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False), nn.BatchNorm3d(planes * block.expansion))\n \n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n \n        return nn.Sequential(*layers)\n \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n \n        x = self.avgpool(x)\n \n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n \n        return x\n\n# def get_fine_tuning_parameters(model, ft_begin_index):\n#     if ft_begin_index == 0:\n#         return model.parameters()\n \n#     ft_module_names = []\n#     for i in range(ft_begin_index, 5):\n#         ft_module_names.append('layer{}'.format(i))\n#     ft_module_names.append('fc')\n \n#     parameters = []\n#     for k, v in model.named_parameters():\n#         for ft_module in ft_module_names:\n#             if ft_module in k:\n#                 parameters.append({'params': v})\n#                 break\n#         else:\n#             parameters.append({'params': v, 'lr': 0.0})\n \n#     return parameters\n\n\ndef resnet18(**kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    return model\n\ndef resnet34(**kwargs):\n    \"\"\"Constructs a ResNet-34 model.\n    \"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:54:25.849407Z","iopub.execute_input":"2021-12-06T05:54:25.849891Z","iopub.status.idle":"2021-12-06T05:54:25.912441Z","shell.execute_reply.started":"2021-12-06T05:54:25.849833Z","shell.execute_reply":"2021-12-06T05:54:25.91175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = resnet34(num_classes=2, sample_size=224, sample_duration=32)\n# model.load_state_dict(torch.load(modeldict['path']))\nif torch.cuda.is_available():\n    model = model.cuda() \nmodel","metadata":{"execution":{"iopub.status.busy":"2021-12-06T06:21:39.294493Z","iopub.execute_input":"2021-12-06T06:21:39.294794Z","iopub.status.idle":"2021-12-06T06:21:40.210034Z","shell.execute_reply.started":"2021-12-06T06:21:39.294746Z","shell.execute_reply":"2021-12-06T06:21:40.209359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mult_ratio = fake_ct//real_ct\nprint(mult_ratio)\n\ntrain_loader = DataLoader(train_fnames, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T06:30:25.657359Z","iopub.execute_input":"2021-12-06T06:30:25.657684Z","iopub.status.idle":"2021-12-06T06:30:25.664366Z","shell.execute_reply.started":"2021-12-06T06:30:25.657631Z","shell.execute_reply":"2021-12-06T06:30:25.66151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_fnames, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T14:37:52.45671Z","iopub.execute_input":"2021-12-06T14:37:52.457031Z","iopub.status.idle":"2021-12-06T14:37:52.461818Z","shell.execute_reply.started":"2021-12-06T14:37:52.456968Z","shell.execute_reply":"2021-12-06T14:37:52.460921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train(model, epochs, output=False, debug_batch_interval=5):\n    optimizer = torch.optim.Adam(model.parameters(),lr=0.0000001)\n    \n    model.train()\n    \n    for epoch in tqdm(range(epochs), position=0, desc=\"Epochs\"):\n        print('Epoch', epoch+1)\n        \n        # Mostly used for stats\n        running, batch_running, ct, batch_ct, correct = 0, 0, 0, 0, 0\n        real_ct, fake_ct, correct_real, correct_fake = 0, 0, 0, 0\n        predictions = defaultdict(list)\n        \n        for batch_idx, vid_name in tqdm(enumerate(train_loader), position=1, desc=\"Batches\", total=len(train_fnames)):\n            \n            # Batch size should be 1!\n            assert(len(vid_name) == 1)\n\n            vid_name = vid_name[0]\n\n            # Get the video by cutting out some images\n            video_frames = get_frames(os.path.join(TRAIN_PATH_PRE_FIX, vid_name))\n\n            preds_video = []\n\n            gt_text = get_label(vid_name)\n#             print(\"This sample is -------------\", gt_text)\n            gt_label = 0 if gt_text == 'REAL' else 1\n\n#             try:\n            # Generate our predictions by getting a confidence score of each frame\n            for im in video_frames:\n                optimizer.zero_grad()\n\n                # Done by a strong notebook... might be worth trying out later\n        #             model = HFlipWrapper(model=model)\n                if gt_label == 0:\n\n                    # Sam, I think this will mess up our loss... if we train it on the same REAL images, we will overfit the REALs and it will be memorizing\n                    for aug_num in range(1,mult_ratio):\n                        \n                        im_real = im\n                        with torch.no_grad():\n                            im_real = DeepFace.detectFace(im_real, detector_backend = 'ssd', enforce_detection = False)\n\n                            # Get transforms/variations of our image\n                            transforms = A.Compose([A.Resize(height=112, width=112),\n                                     A.Normalize(), A.ShiftScaleRotate(p=1)])\n\n\n                            im_real = np.array([transforms(image=im_real)['image']\n                                 ])\n\n                            im_real = torch.from_numpy(im_real.transpose([3, 0, 1, 2])).float()\n\n                            # Unsqueeze because we are using 1 frame, not many frames together as a video\n                            im_real = im_real.unsqueeze(0)\n\n                        im_real = torch.tensor(im_real, requires_grad=True)\n                        model.cuda()\n                        im_real = im_real.cuda()\n                        y_pred = model(im_real)\n                        prob0, prob1 = torch.mean(torch.exp(F.log_softmax(y_pred, dim=1)),dim=0)\n\n                        # Add this frame's prediction\n                        preds_video.append(float(prob1))\n\n                        # Take the loss after generating predictions on all images for a given video\n                        loss = F.mse_loss(prob1.reshape(1), torch.tensor(gt_label, dtype=torch.float32).reshape(1).cuda())\n\n#                         pred = 'REAL' if prob1 < 0.5 else 'FAKE'\n#                         print(\"Prediction:\", pred, \"Confidence:\", prob1)\n#                         print(\"Actual:\", gt_text, \"loss:\", loss.item())\n\n                        # If guessed correctly that it was REAL\n                        if prob1 < 0.5: # already know gt_label == 0):\n                            correct += 1\n                            correct_real += 1\n\n                        real_ct += 1\n                        running += loss.item()\n                        batch_running += loss.item()\n                        ct += 1\n                        batch_ct += 1\n                        loss.backward()\n                        optimizer.step()\n\n                else:\n\n                    with torch.no_grad():\n                        # Get transforms/variations of our image\n                        transforms = A.Compose([A.Resize(height=112, width=112),\n                                         A.Normalize()])\n\n                        im = DeepFace.detectFace(im, detector_backend = 'ssd', enforce_detection = False)\n                        im = np.array([transforms(image=im)['image']\n                                     ])\n\n                        im = torch.from_numpy(im.transpose([3, 0, 1, 2])).float()\n\n                        # Unsqueeze because we are using 1 frame, not many frames together as a video\n                        im = im.unsqueeze(0)\n                    \n                    \n                    im = torch.tensor(im, requires_grad=True)\n                    model.cuda()\n                    im = im.cuda()\n                    y_pred = model(im)\n                    prob0, prob1 = torch.mean(torch.exp(F.log_softmax(y_pred, dim=1)),dim=0)\n\n                    # Add this frame's prediction\n                    preds_video.append(float(prob1))\n\n                    # Take the loss after generating predictions on all images for a given video\n                    loss = F.mse_loss(prob1.reshape(1), torch.tensor(gt_label, dtype=torch.float32).reshape(1).cuda())\n\n#                     pred = 'REAL' if prob1 < 0.5 else 'FAKE'\n#                     print(\"Prediction:\", pred, \"Confidence (REAL, FAKE):\", prob0.item(), prob1.item())\n#                     print(\"Actual:\", gt_text, \"loss:\", loss.item())\n\n                    if prob1 >= 0.5: # already know that gt_label == 1):\n                        correct += 1\n                        correct_fake += 1\n                    \n                    fake_ct += 1\n                    running += loss.item()\n                    batch_running += loss.item()\n                    ct += 1\n                    batch_ct += 1\n                    loss.backward()\n                    optimizer.step()\n\n            # Every debug_batch_interval iterations, print the data we've churned through (iterations * data per batch)\n            if output and batch_idx % (len(train_fnames) // debug_batch_interval) == 0 and batch_idx != 0:                \n                print('Epoch: {} [{}/{} ({:.2f}%)]\\tBatch Loss: {:.5f}\\tEpoch Loss: {:.5f}'.format(\n                          epoch+1, batch_idx, len(train_fnames),    # current sample num / total num\n                          100. * batch_idx / len(train_fnames), # this batch num's % of total dataset\n                          batch_running / batch_ct, # the loss for this batch\n                          running / ct) # running loss for the epoch\n                     )\n                batch_running, batch_ct = 0, 0\n\n            # As long as we have predictions from our images, take the mean of those predictions to determine a prediction for the video\n            if preds_video:\n                predictions[vid_name].extend([np.mean(preds_video)])\n\n#             except Exception as e:\n#                 print(f\"ERROR: Video {vid_name}: {e}\")\n                \n        if ct == 0:\n            continue\n        this_loss = running / ct\n        if output:\n            print(\"\\nAverage Loss:\", round(running / ct * 10000) / 10000.0,\"\\n\")\n        else:\n            print(\"Epoch\", epoch+1, \"Average Loss:\", round(this_loss * 10000) / 10000.0)\n        print(\"Accuracy =\", str(round(correct / ct * 100) / 100.0) + \"%\", \"(\" + str(correct), \"/\" , str(ct) + \")\")\n        print(\"Real vids:\", str(round(correct_real / real_ct * 100) / 100.0) + \"%\", \"(\" + str(correct_real), \"/\" , str(real_ct) + \")\")\n        print(\"Fake vids:\", str(round(correct_fake / fake_ct * 100) / 100.0) + \"%\", \"(\" + str(correct_fake), \"/\" , str(fake_ct) + \")\")","metadata":{"execution":{"iopub.status.busy":"2021-12-06T07:36:32.125126Z","iopub.execute_input":"2021-12-06T07:36:32.125501Z","iopub.status.idle":"2021-12-06T07:36:32.162184Z","shell.execute_reply.started":"2021-12-06T07:36:32.125402Z","shell.execute_reply":"2021-12-06T07:36:32.161196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(model, 18, output=True)\n'''\nepoch 1\nAccuracy = 63.15% (3499 / 5540)\nReal vids: 82.94% (1916 / 2310)\nFake vids: 49% (1583 / 3230)\n'''","metadata":{"execution":{"iopub.status.busy":"2021-12-06T07:36:34.247577Z","iopub.execute_input":"2021-12-06T07:36:34.247872Z","iopub.status.idle":"2021-12-06T14:23:57.798104Z","shell.execute_reply.started":"2021-12-06T07:36:34.247824Z","shell.execute_reply":"2021-12-06T14:23:57.796815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = {'model': resnet34(num_classes=2, sample_size=224, sample_duration=32),\n             'state_dict': model.state_dict(),\n#              'optimizer': optimizer.state_dict()\n             }\ntorch.save(checkpoint, 'checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T14:24:30.97029Z","iopub.execute_input":"2021-12-06T14:24:30.970602Z","iopub.status.idle":"2021-12-06T14:24:32.30279Z","shell.execute_reply.started":"2021-12-06T14:24:30.970546Z","shell.execute_reply":"2021-12-06T14:24:32.301944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(filepath='checkpoint.pth'):\n    # To load again\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n\n    # If using for testing\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    model.eval()\n\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef test(model):\n    model.eval()\n    \n    # Mostly used for stats\n    running, batch_running, ct, batch_ct, correct = 0, 0, 0, 0, 0\n    real_ct, fake_ct, correct_real, correct_fake = 0, 0, 0, 0\n    predictions = defaultdict(list)\n        \n    for batch_idx, vid_name in tqdm(enumerate(test_loader), position=1, desc=\"Batches\", total=len(train_fnames)):\n\n        # Batch size should be 1!\n        assert(len(vid_name) == 1)\n\n        vid_name = vid_name[0]\n\n        # Get the video by cutting out some images\n        video_frames = get_frames(os.path.join(TRAIN_PATH_PRE_FIX, vid_name))\n\n        preds_video = []\n\n        gt_text = get_label(vid_name)\n        gt_label = 0 if gt_text == 'REAL' else 1\n\n        # Generate our predictions by getting a confidence score of each frame\n        for im in video_frames:\n\n            # Done by a strong notebook... might be worth trying out later\n    #             model = HFlipWrapper(model=model)\n            if gt_label == 0:\n\n                im_real = im\n                with torch.no_grad():\n                    im_real = DeepFace.detectFace(im_real, detector_backend = 'ssd', enforce_detection = False)\n\n                    # Get transforms/variations of our image\n                    transforms = A.Compose([A.Resize(height=112, width=112),\n                             A.Normalize()])\n\n                    im_real = torch.from_numpy(im_real.transpose([3, 0, 1, 2])).float()\n\n                    # Unsqueeze because we are using 1 frame, not many frames together as a video\n                    im_real = im_real.unsqueeze(0)\n\n                y_pred = model(im_real)\n                prob0, prob1 = torch.mean(torch.exp(F.log_softmax(y_pred, dim=1)),dim=0)\n\n                # Add this frame's prediction\n                preds_video.append(float(prob1))\n\n                # Take the loss after generating predictions on all images for a given video\n                loss = F.mse_loss(prob1.reshape(1), torch.tensor(gt_label, dtype=torch.float32).reshape(1).cuda())\n\n                # If guessed correctly that it was REAL\n                if prob1 < 0.5: # already know gt_label == 0):\n                    correct += 1\n                    correct_real += 1\n\n                real_ct += 1\n                running += loss.item()\n                ct += 1\n\n            else:\n\n                with torch.no_grad():\n                    # Get transforms/variations of our image\n                    transforms = A.Compose([A.Resize(height=112, width=112),\n                                     A.Normalize()])\n\n                    im = DeepFace.detectFace(im, detector_backend = 'ssd', enforce_detection = False)\n\n                    im = torch.from_numpy(im.transpose([3, 0, 1, 2])).float()\n\n                    # Unsqueeze because we are using 1 frame, not many frames together as a video\n                    im = im.unsqueeze(0)\n\n\n                y_pred = model(im)\n                prob0, prob1 = torch.mean(torch.exp(F.log_softmax(y_pred, dim=1)),dim=0)\n\n                # Add this frame's prediction\n                preds_video.append(float(prob1))\n\n                # Take the loss after generating predictions on all images for a given video\n                loss = F.mse_loss(prob1.reshape(1), torch.tensor(gt_label, dtype=torch.float32).reshape(1).cuda())\n\n                if prob1 >= 0.5: # already know that gt_label == 1):\n                    correct += 1\n                    correct_fake += 1\n\n                fake_ct += 1\n                running += loss.item()\n                ct += 1\n\n        # As long as we have predictions from our images, take the mean of those predictions to determine a prediction for the video\n        if preds_video:\n            predictions[vid_name].extend([np.mean(preds_video)])\n\n#             except Exception as e:\n#                 print(f\"ERROR: Video {vid_name}: {e}\")\n\n    if ct == 0:\n        return\n    this_loss = running / ct\n    print(\"\\nAverage Loss:\", round(running / ct * 10000) / 10000.0,\"\\n\")\n    print(\"Accuracy =\", str(round(correct / ct * 100) / 100.0) + \"%\", \"(\" + str(correct), \"/\" , str(ct) + \")\")\n    print(\"Real vids:\", str(round(correct_real / real_ct * 100) / 100.0) + \"%\", \"(\" + str(correct_real), \"/\" , str(real_ct) + \")\")\n    print(\"Fake vids:\", str(round(correct_fake / fake_ct * 100) / 100.0) + \"%\", \"(\" + str(correct_fake), \"/\" , str(fake_ct) + \")\")\n    return correct / ct","metadata":{"execution":{"iopub.status.busy":"2021-12-06T14:37:17.070226Z","iopub.execute_input":"2021-12-06T14:37:17.07054Z","iopub.status.idle":"2021-12-06T14:37:17.099091Z","shell.execute_reply.started":"2021-12-06T14:37:17.070482Z","shell.execute_reply":"2021-12-06T14:37:17.098256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No way to test this easily\ntest(model)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T14:38:01.677797Z","iopub.execute_input":"2021-12-06T14:38:01.678096Z","iopub.status.idle":"2021-12-06T14:38:01.765759Z","shell.execute_reply.started":"2021-12-06T14:38:01.678045Z","shell.execute_reply":"2021-12-06T14:38:01.764426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k,v in predictions.items():\n    string = '{} : {:.4f} //'.format(k, np.mean(v))\n    for proba in v:\n        string += ' {:.4f}'.format(proba)\n    print(string)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T05:54:34.841471Z","iopub.status.idle":"2021-12-06T05:54:34.84221Z"},"trusted":true},"execution_count":null,"outputs":[]}]}